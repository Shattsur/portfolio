{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞\" data-toc-modified-id=\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞</a></span><ul class=\"toc-item\"><li><span><a href=\"#–û–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏–µ-—Å-–¥–∞–Ω–Ω—ã–º–∏\" data-toc-modified-id=\"–û–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏–µ-—Å-–¥–∞–Ω–Ω—ã–º–∏-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>–û–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏</a></span></li><li><span><a href=\"#–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞-–¥–∞–Ω–Ω—ã—Ö\" data-toc-modified-id=\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞-–¥–∞–Ω–Ω—ã—Ö-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö</a></span></li></ul></li><li><span><a href=\"#–û–±—É—á–µ–Ω–∏–µ\" data-toc-modified-id=\"–û–±—É—á–µ–Ω–∏–µ-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>–û–±—É—á–µ–Ω–∏–µ</a></span><ul class=\"toc-item\"><li><span><a href=\"#–û–±—É—á–µ–Ω–∏–µ-–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö-–º–æ–¥–µ–ª–µ–π\" data-toc-modified-id=\"–û–±—É—á–µ–Ω–∏–µ-–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö-–º–æ–¥–µ–ª–µ–π-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π</a></span><ul class=\"toc-item\"><li><span><a href=\"#–í—ã–≤–æ–¥—ã-–ø–æ-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\" data-toc-modified-id=\"–í—ã–≤–æ–¥—ã-–ø–æ-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span><strong>–í—ã–≤–æ–¥—ã –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤</strong></a></span></li></ul></li><li><span><a href=\"#–û–±—É—á–µ–Ω–∏–µ-–º–æ–¥–µ–ª–∏-BERT\" data-toc-modified-id=\"–û–±—É—á–µ–Ω–∏–µ-–º–æ–¥–µ–ª–∏-BERT-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT</a></span><ul class=\"toc-item\"><li><span><a href=\"#–ê–Ω–∞–ª–∏–∑-–º–æ–¥–µ–ª–∏-BERT\" data-toc-modified-id=\"–ê–Ω–∞–ª–∏–∑-–º–æ–¥–µ–ª–∏-BERT-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>–ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏ BERT</a></span></li></ul></li></ul></li><li><span><a href=\"#–ò—Ç–æ–≥–æ–≤—ã–µ-–≤—ã–≤–æ–¥—ã\" data-toc-modified-id=\"–ò—Ç–æ–≥–æ–≤—ã–µ-–≤—ã–≤–æ–¥—ã-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>–ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã</a></span></li><li><span><a href=\"#–ß–µ–∫-–ª–∏—Å—Ç-–ø—Ä–æ–≤–µ—Ä–∫–∏\" data-toc-modified-id=\"–ß–µ–∫-–ª–∏—Å—Ç-–ø—Ä–æ–≤–µ—Ä–∫–∏-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>–ß–µ–∫-–ª–∏—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–µ–∫—Ç –¥–ª—è ¬´–í–∏–∫–∏—à–æ–ø¬ª c BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω ¬´–í–∏–∫–∏—à–æ–ø¬ª –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å. –¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤, –∫–∞–∫ –≤ –≤–∏–∫–∏-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö. –¢–æ –µ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö. –ú–∞–≥–∞–∑–∏–Ω—É –Ω—É–∂–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∏—Ö –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é. \n",
    "\n",
    "–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ. –í –≤–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π –æ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–æ–∫.\n",
    "\n",
    "–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –º–æ–¥–µ–ª—å —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ *F1* –Ω–µ –º–µ–Ω—å—à–µ 0.75. \n",
    "\n",
    "**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞**\n",
    "\n",
    "1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.\n",
    "2. –û–±—É—á–∏—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏. \n",
    "3. –°–¥–µ–ª–∞–π—Ç–µ –≤—ã–≤–æ–¥—ã.\n",
    "\n",
    "–î–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å *BERT* –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å.\n",
    "\n",
    "**–û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**\n",
    "\n",
    "–î–∞–Ω–Ω—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ñ–∞–π–ª–µ `toxic_comments.csv`. –°—Ç–æ–ª–±–µ—Ü *text* —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è, –∞ *toxic* ‚Äî —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SelectKBest, chi2\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# –ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∞–∑–æ–≤—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (f1_score, classification_report, \n",
    "                           confusion_matrix, precision_recall_curve)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# –ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification,\n",
    "                         Trainer, TrainingArguments, AutoModelForSequenceClassification,\n",
    "                         AutoTokenizer, get_linear_schedule_with_warmup,\n",
    "                         get_cosine_schedule_with_warmup, AutoConfig)\n",
    "\n",
    "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import loguniform, randint\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import swifter  # –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è apply\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)  \n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "RANDOM_STATE = 42\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# –§–∏–∫—Å–∞—Ü–∏—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "else:\n",
    "    print(\"CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§–∞–π–ª /data/toxic_comments.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ https://code.s3.yandex.net/datasets/toxic_comments.csv...\n",
      "–í—Å–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ.\n"
     ]
    }
   ],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º\n",
    "pth_df = '/data/toxic_comments.csv'                  \n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º URL –¥–ª—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏\n",
    "url_df = 'https://code.s3.yandex.net/datasets/toxic_comments.csv'\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "def load_data(local_path, url, sep=','):\n",
    "    if os.path.exists(local_path):\n",
    "        print(f'–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {local_path}...')\n",
    "        return pd.read_csv(local_path, sep=sep)\n",
    "    else:\n",
    "        print(f'–§–∞–π–ª {local_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {url}...')\n",
    "        return pd.read_csv(url, sep=sep)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "try:\n",
    "    df = load_data(pth_df, url_df)\n",
    "    print('–í—Å–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ.')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/toxic_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "display(df.head())  # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫\n",
    "print(\"–ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "df.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ DataFrame:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£–¥–∞–ª–∏–º —Å—Ç–æ–ª–±–µ—Ü Unnamed: 0\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö**  \n",
    "   - –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞ `toxic_comments.csv`.  \n",
    "   - –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: **159 292 —Å—Ç—Ä–æ–∫–∏ √ó 3 —Å—Ç–æ–ª–±—Ü–∞**.  \n",
    "   - –ü—Ä–æ–ø—É—Å–∫–∏ –∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.\n",
    "   - \n",
    "2. **–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö**  \n",
    "   - `Unnamed: 0` ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å—Ç–æ–ª–±–µ—Ü (—É–¥–∞–ª–µ–Ω).  \n",
    "   - `text` ‚Äî —Ç–µ–∫—Å—Ç—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (—Ç–∏–ø `object`).  \n",
    "   - `toxic` ‚Äî –±–∏–Ω–∞—Ä–Ω—ã–π —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ (10.16% ‚Äî —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏).\n",
    "   - \n",
    "3. **–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏**  \n",
    "   - –ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è **–¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤** (90.84% vs 10.16%).  \n",
    "   - –¢—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–æ—á–∏—Å—Ç–∫–∞, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –≤—ã–ø–æ–ª–Ω–∏–º –±–∞–∑–æ–≤—É—é –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–∞–∂–Ω—ã—Ö –¥–ª—è BERT —ç–ª–µ–º–µ–Ω—Ç–æ–≤:\n",
    "    - –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è\n",
    "    - —ç–º–æ—Ç–∏–∫–æ–Ω—ã\n",
    "    - —Ä–µ–≥–∏—Å—Ç—Ä (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è cased-–º–æ–¥–µ–ª—å)\n",
    "    \"\"\"\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ URL\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "  \n",
    "    # –ó–∞–º–µ–Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫ –∏ —Ç–∞–±—É–ª—è—Ü–∏–π –Ω–∞ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'[\\n\\t\\r]+', ' ', text)\n",
    "\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ IP-–∞–¥—Ä–µ—Å–æ–≤\n",
    "    text = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', '[IP]', text)\n",
    "\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ email-–∞–¥—Ä–µ—Å–æ–≤\n",
    "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤, –∫—Ä–æ–º–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã—Ö\n",
    "    text = re.sub(r'[^\\w\\s.,!?–∞-—è–ê-–Ø—ë–Å-]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É –∫ –¥–∞–Ω–Ω—ã–º\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü\n",
    "df = df.drop(columns=['text'])\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç POS-—Ç–µ–≥–∏ –∏–∑ Penn Treebank –≤ —Ñ–æ—Ä–º–∞—Ç WordNet.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º\n",
    "\n",
    "def preprocess_classic(text):\n",
    "    \"\"\"\n",
    "    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π:\n",
    "    - –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    - —É–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤\n",
    "    - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    - –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º POS-—Ç–µ–≥–æ–≤\n",
    "    - —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "    \"\"\"\n",
    "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    text = text.lower()\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä –∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n",
    "    \n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ POS-—Ç–µ–≥–æ–≤\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º POS-—Ç–µ–≥–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "    processed_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        if word not in stop_words and word.isalpha():  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –Ω–µ-—Å–ª–æ–≤–∞\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "            processed_words.append(lemma)\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "df['text_classic'] = df['text_clean'].swifter.apply(preprocess_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ df['text_clean'] –∏ df['text_classic'] –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏:\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"BERT: {df['text_clean'].iloc[i]}\\nClassic: {df['text_classic'].iloc[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∏ –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏\n",
    "toxic_texts = df[df['toxic'] == 1]['text_classic']\n",
    "non_toxic_texts = df[df['toxic'] == 0]['text_classic']\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤\n",
    "def generate_wordcloud(texts, title):\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        max_words=100\n",
    "    ).generate(' '.join(texts))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# –û–±–ª–∞–∫–æ —Å–ª–æ–≤ –¥–ª—è —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n",
    "generate_wordcloud(toxic_texts, '–ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö')\n",
    "\n",
    "# –û–±–ª–∞–∫–æ —Å–ª–æ–≤ –¥–ª—è –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n",
    "generate_wordcloud(non_toxic_texts, '–ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö')\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤\n",
    "def get_top_words(texts, n=20):\n",
    "    words = ' '.join(texts).split()\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "# –¢–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n",
    "top_toxic_words = get_top_words(toxic_texts)\n",
    "print(\"–¢–æ–ø-20 —Å–ª–æ–≤ –≤ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö:\")\n",
    "for word, count in top_toxic_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# –¢–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n",
    "top_non_toxic_words = get_top_words(non_toxic_texts)\n",
    "print(\"\\n–¢–æ–ø-20 —Å–ª–æ–≤ –≤ –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö:\")\n",
    "for word, count in top_non_toxic_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "\n",
    "# –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "texts_for_bert = df['text_clean']       # –û—á–∏—â–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (–¥–ª—è BERT)\n",
    "texts_for_classic = df['text_classic']  # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π)\n",
    "labels = df['toxic']                    # –ú–µ—Ç–∫–∏ (–æ–±—â–∏–µ –¥–ª—è –æ–±–æ–∏—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π)\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è BERT –º–æ–¥–µ–ª–∏\n",
    "\n",
    "X_bert_train, X_bert_test, y_bert_train, y_bert_test = train_test_split(\n",
    "    texts_for_bert,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True,\n",
    "    stratify=labels  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤\n",
    ")\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π (LogisticRegression, RandomForest –∏ —Ç.–¥.)\n",
    "\n",
    "X_classic_train, X_classic_test, y_classic_train, y_classic_test = train_test_split(\n",
    "    texts_for_classic,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, lower case)\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True,\n",
    "    stratify=labels  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤\n",
    ")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "\n",
    "print(f\"\\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –≤—ã–±–æ—Ä–æ–∫:\")\n",
    "print(f\"BERT Train: {len(X_bert_train)}, Test: {len(X_bert_test)}\")\n",
    "print(f\"Classic Train: {len(X_classic_train)}, Test: {len(X_classic_test)}\")\n",
    "\n",
    "print(f\"\\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫:\")\n",
    "print(f\"BERT Train - –¥–æ–ª—è —Ç–æ–∫—Å–∏—á–Ω—ã—Ö: {y_bert_train.mean():.4f}\")\n",
    "print(f\"BERT Test - –¥–æ–ª—è —Ç–æ–∫—Å–∏—á–Ω—ã—Ö: {y_bert_test.mean():.4f}\")\n",
    "print(f\"Classic Train - –¥–æ–ª—è —Ç–æ–∫—Å–∏—á–Ω—ã—Ö: {y_classic_train.mean():.4f}\")\n",
    "print(f\"Classic Test - –¥–æ–ª—è —Ç–æ–∫—Å–∏—á–Ω—ã—Ö: {y_classic_test.mean():.4f}\")\n",
    "\n",
    "# –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ—Ç–æ–∫\n",
    "assert all(y_bert_train == y_classic_train)\n",
    "assert all(y_bert_test == y_classic_test)\n",
    "print(\"\\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ—Ç–æ–∫ –ø—Ä–æ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
    "# tfidf = TfidfVectorizer(\n",
    "#     max_features=50000,  \n",
    "#     ngram_range=(1, 1),  \n",
    "#     stop_words='english',\n",
    "#     sublinear_tf=True    \n",
    "# )\n",
    "\n",
    "# X_classic_train_tfidf = tfidf.fit_transform(X_classic_train)\n",
    "# X_classic_test_tfidf = tfidf.transform(X_classic_test)\n",
    "\n",
    "# print(f\"\\n–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å TF-IDF –º–∞—Ç—Ä–∏—Ü—ã: {X_classic_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—É–¥–∞–ª–µ–Ω–∏–µ IP-–∞–¥—Ä–µ—Å–æ–≤ –∏ —Ç.–ø.)\n",
    "def advanced_clean(text):\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ IP-–∞–¥—Ä–µ—Å–æ–≤\n",
    "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def advanced_clean_bert(text):\n",
    "    # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ IP-–∞–¥—Ä–µ—Å–∞ –∏ email (–æ—Å—Ç–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º)\n",
    "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)  # IP\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Email\n",
    "    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã (–Ω–æ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∫ text_clean (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è BERT)\n",
    "df['text_clean'] = df['text_clean'].apply(advanced_clean_bert)\n",
    "\n",
    "# –î–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π (TF-IDF) –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å —Å—Ç–∞—Ä—É—é –æ—á–∏—Å—Ç–∫—É\n",
    "df['text_classic'] = df['text_classic'].apply(advanced_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Pipeline –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "\n",
    "# 1. LogReg + Feature Selection —Å TF-IDF –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ\n",
    "logreg_fs = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=50000,\n",
    "        ngram_range=(1, 1),\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "    ('feature_selection', SelectKBest(chi2)),\n",
    "    ('model', SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        penalty='elasticnet',\n",
    "        class_weight='balanced',\n",
    "        max_iter=2000,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg_params = {\n",
    "    'tfidf__max_features': [30000, 50000],\n",
    "    'feature_selection__k': [10000, 15000, 20000],\n",
    "    'model__alpha': [0.0001, 0.001, 0.01],\n",
    "    'model__l1_ratio': [0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "# 2. LightGBM —Å TF-IDF –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ\n",
    "lgbm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=50000,\n",
    "        ngram_range=(1, 1),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('lgbm', LGBMClassifier(\n",
    "        objective='binary',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        force_row_wise=True,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "lgbm_params = {\n",
    "    'lgbm__num_leaves': [31, 63, 127],\n",
    "    'lgbm__max_depth': [5, 7, 10],\n",
    "    'lgbm__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'lgbm__n_estimators': [200, 300, 500],\n",
    "    'lgbm__min_child_samples': [20, 50],\n",
    "    'lgbm__reg_alpha': [0, 0.1],\n",
    "    'lgbm__reg_lambda': [0, 0.1],\n",
    "    'lgbm__class_weight': ['balanced', {0: 1, 1: 3}]\n",
    "}\n",
    "\n",
    "# 3. LinearSVC —Å TF-IDF –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ\n",
    "svm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=50000,\n",
    "        ngram_range=(1, 1),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('model', LinearSVC(\n",
    "        class_weight={0: 1, 1: 2.5},\n",
    "        random_state=RANDOM_STATE,\n",
    "        dual=False,\n",
    "        loss='squared_hinge',\n",
    "        max_iter=2000\n",
    "    ))\n",
    "])\n",
    "\n",
    "svm_params = {\n",
    "    'model__C': [0.3, 0.5, 0.7],\n",
    "    'model__tol': [1e-4, 1e-5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(pipeline, params, X_train, y_train, name):\n",
    "    \"\"\"–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\"\"\"\n",
    "    print(f\"\\n=== –û–±—É—á–µ–Ω–∏–µ {name} ===\")\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        params,\n",
    "        n_iter=5,\n",
    "        scoring='f1',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    search.fit(X_train, y_train)\n",
    "    training_time = time.time() - start\n",
    "    \n",
    "    print(f\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {search.best_params_}\")\n",
    "    print(f\"–õ—É—á—à–∏–π F1 –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {search.best_score_:.4f}\")\n",
    "    print(f\"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.1f} —Å–µ–∫\")\n",
    "    \n",
    "    return search.best_estimator_, search.best_score_, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏)\n",
    "models = {\n",
    "    'LogReg': (logreg_fs, logreg_params),\n",
    "    'LightGBM': (lgbm, lgbm_params),\n",
    "    'LinearSVM': (svm, svm_params)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_f1 = 0\n",
    "best_model_name = ''\n",
    "best_model = None\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    best_current_model, cv_f1, t_time = train_model(\n",
    "        model, params,\n",
    "        X_classic_train, y_classic_train,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\n",
    "        name\n",
    "    )\n",
    "    results[name] = {\n",
    "        'model': best_current_model,\n",
    "        'cv_f1': cv_f1,\n",
    "        'time': t_time\n",
    "    }\n",
    "    \n",
    "    if cv_f1 > best_f1:\n",
    "        best_f1 = cv_f1\n",
    "        best_model_name = name\n",
    "        best_model = best_current_model\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "print(\"\\n–ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏:\")\n",
    "print(f\"{'–ú–æ–¥–µ–ª—å':<15} {'F1-–º–µ—Ä–∞ (CV)':<12} {'–í—Ä–µ–º—è (—Å–µ–∫)':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:<15} {result['cv_f1']:<12.4f} {result['time']:<10.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"–û—Ü–µ–Ω–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ ({best_model_name}) –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:\")\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "y_pred = best_model.predict(X_classic_test)\n",
    "test_f1 = f1_score(y_classic_test, y_pred)\n",
    "\n",
    "print(classification_report(y_classic_test, y_pred, digits=4))\n",
    "print(f\"F1 –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_f1:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º confusion matrix\n",
    "cm = confusion_matrix(y_classic_test, y_pred)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã\n",
    "print(\"=\"*50)\n",
    "print(\"Confusion Matrix Raw Data:\")\n",
    "print(cm)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_classic_test, y_pred, target_names=['Non-Toxic', 'Toxic']))\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                 cbar=False, linewidths=0.5, linecolor='gray')\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ–¥–ø–∏—Å–∏\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.xaxis.set_ticklabels(['Non-Toxic', 'Toxic'], fontsize=10)\n",
    "ax.yaxis.set_ticklabels(['Non-Toxic', 'Toxic'], fontsize=10)\n",
    "ax.set_title('Confusion Matrix for Toxic Comments Detection', fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Matrix Interpretation:\")\n",
    "print(f\"True Negatives (TN): {cm[0,0]} - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏\")\n",
    "print(f\"False Positives (FP): {cm[0,1]} - –ù–µ—Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –æ—à–∏–±–æ—á–Ω–æ –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ —Ç–æ–∫—Å–∏—á–Ω—ã–µ\")\n",
    "print(f\"False Negatives (FN): {cm[1,0]} - –¢–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª—å—é\")\n",
    "print(f\"True Positives (TP): {cm[1,1]} - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **–í—ã–≤–æ–¥—ã –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤**\n",
    "\n",
    " 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "| –ú–æ–¥–µ–ª—å      | F1-–º–µ—Ä–∞ (CV) | –í—Ä–µ–º—è (—Å–µ–∫) |\n",
    "|-------------|--------------|-------------|\n",
    "| **LogReg**  | 0.7063       | 9.0         |\n",
    "| **LightGBM**| 0.7645       | 53.0        |\n",
    "| **LinearSVM**| **0.7835**  | **8.4**     |\n",
    "\n",
    "- **LinearSVM** –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (F1=0.7835) –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è (8.4 —Å–µ–∫).\n",
    "- LightGBM —É—Å—Ç—É–ø–∞–µ—Ç –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ (53 —Å–µ–∫), –Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç LogReg –ø–æ –∫–∞—á–µ—Å—Ç–≤—É.\n",
    "- –õ–∏–Ω–µ–π–Ω—ã–µ –º–µ—Ç–æ–¥—ã (SVM, LogReg) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å TF-IDF.\n",
    "\n",
    "---\n",
    "\n",
    " 2. –ö–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "**–î–ª—è –∫–ª–∞—Å—Å–∞ Toxic (1):**\n",
    "- **Precision = 0.81** ‚Üí –ò–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ 81% –≤–µ—Ä–Ω—ã.\n",
    "- **Recall = 0.75** ‚Üí –ü—Ä–æ–ø—É—â–µ–Ω–æ **25%** —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (FN=817).\n",
    "- **F1 = 0.7783** ‚Üí –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–Ω–æ—Ç–æ–π.\n",
    "\n",
    "**–î–ª—è –∫–ª–∞—Å—Å–∞ Non-Toxic (0):**\n",
    "- –ò–¥–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (F1=0.98) –∏–∑-–∑–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –¥–∞–Ω–Ω—ã—Ö (28.6k vs 3.2k).\n",
    "\n",
    "---\n",
    "\n",
    " 3. –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–æ–Ω—ã\n",
    "- **False Negatives (FN)**: 817 —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã (—Ä–∏—Å–∫ –ø—Ä–æ–ø—É—Å–∫–∞ –≤—Ä–µ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞).\n",
    "- **False Positives (FP)**: 562 –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –ø–æ–º–µ—á–µ–Ω—ã –∫–∞–∫ —Ç–æ–∫—Å–∏—á–Ω—ã–µ (–ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –º–æ–¥–µ—Ä–∞—Ç–æ—Ä–æ–≤).\n",
    "\n",
    "---\n",
    "\n",
    " 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "1. **–£–ª—É—á—à–µ–Ω–∏–µ Recall –¥–ª—è Toxic –∫–ª–∞—Å—Å–∞**:\n",
    "   - –ü–æ–≤—ã—Å–∏—Ç—å –≤–µ—Å –∫–ª–∞—Å—Å–∞ 1 (`class_weight`).\n",
    "   - –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ç–µ—Ö–Ω–∏–∫–∏ oversampling (SMOTE) –∏–ª–∏ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ.\n",
    "   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (threshold tuning).\n",
    "\n",
    "2. **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã**:\n",
    "   - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å BERT/Transformer-–º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.\n",
    "   - –î–æ–±–∞–≤–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏).\n",
    "\n",
    "3. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**:\n",
    "   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LightGBM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "\n",
    "---\n",
    "\n",
    " 5. –†–∏—Å–∫–∏\n",
    "- **–í—ã—Å–æ–∫–∏–π FP**: –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ç—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Ä—É—á–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É.\n",
    "- **–ù–∏–∑–∫–∏–π Recall –¥–ª—è Toxic**: –†–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.\n",
    "\n",
    "---\n",
    "\n",
    "**–ò—Ç–æ–≥**: LinearSVM ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –î–ª—è production-—Å–∏—Å—Ç–µ–º—ã —Ç—Ä–µ–±—É–µ—Ç—Å—è:  \n",
    "‚úÖ **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: –°–Ω–∏–∂–µ–Ω–∏–µ FN (—É–ª—É—á—à–µ–Ω–∏–µ Recall –¥–ª—è Toxic).  \n",
    "‚ö†Ô∏è **–ö–æ–º–ø—Ä–æ–º–∏—Å—Å**: –í–æ–∑–º–æ–∂–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ FP –ø—Ä–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ.  \n",
    "üîç **–î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ**: –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π (BERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True  # –í–∫–ª—é—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é cuDNN\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # –†–∞–∑—Ä–µ—à–∞–µ–º TensorFloat-32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets, weights):\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—é –±–µ–∑ reduction\n",
    "        loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ –≤–µ—Å–∞ –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º\n",
    "        return (loss * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBertDataset(Dataset):\n",
    "    def __init__(self, texts, labels, weights, tokenizer, max_len):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.weights = weights\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'weights': self.weights[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_data(texts, labels, tokenizer, max_len=128):\n",
    "    # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "    classes = np.unique(labels)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
    "    sample_weights = torch.tensor(\n",
    "        [class_weights[list(classes).index(label)] for label in labels], \n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    return WeightedBertDataset(texts, labels, sample_weights, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π\n",
    "model_name = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = 2  # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "config.hidden_dropout_prob = 0.1\n",
    "config.attention_probs_dropout_prob = 0.1\n",
    "\n",
    "# 3. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä\n",
    ").to(device)\n",
    "\n",
    "# 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "import torch.nn as nn\n",
    "model.classifier = nn.Linear(768, 2).to(device)  # –Ø–≤–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "print(\"–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—É—á–µ–Ω–∏—é!\")\n",
    "print(f\"–í—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã: {model.num_parameters()/1e6:.1f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "BATCH_SIZE = 64  \n",
    "GRAD_ACCUM_STEPS = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = prepare_bert_data(X_bert_train, y_bert_train, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,  \n",
    "    persistent_workers=False  \n",
    ")\n",
    "\n",
    "test_dataset = prepare_bert_data(X_bert_test, y_bert_test, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE*2,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ======== –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è ========\n",
    "scaler = GradScaler()  # –î–ª—è mixed precision\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=2e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Å–ª–æ–∏ (–∫—Ä–æ–º–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞) –Ω–∞ –ø–µ—Ä–≤—ã—Ö —ç–ø–æ—Ö–∞—Ö\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS // GRAD_ACCUM_STEPS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1*total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "criterion = WeightedLoss()\n",
    "\n",
    "best_f1 = 0\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤—Å–µ —Å–ª–æ–∏ –ø–æ—Å–ª–µ 1-–π —ç–ø–æ—Ö–∏\n",
    "    if epoch == 1:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # ===== –û–±—É—á–µ–Ω–∏–µ =====\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    train_preds, train_truths = [], []\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device)\n",
    "            )\n",
    "            loss = criterion(\n",
    "                outputs.logits,\n",
    "                batch['labels'].to(device),\n",
    "                batch['weights'].to(device)\n",
    "            ) / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % GRAD_ACCUM_STEPS == 0 or (i + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "        train_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        train_truths.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_f1 = f1_score(train_truths, train_preds, pos_label=1)\n",
    "    \n",
    "    # ===== –í–∞–ª–∏–¥–∞—Ü–∏—è =====\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_truths = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device)\n",
    "                )\n",
    "                val_loss += criterion(\n",
    "                    outputs.logits,\n",
    "                    batch['labels'].to(device),\n",
    "                    batch['weights'].to(device)\n",
    "                ).item()\n",
    "            val_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            val_truths.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    val_f1 = f1_score(val_truths, val_preds, pos_label=1)\n",
    "    \n",
    "    # ===== –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ =====\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # ===== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ =====\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_bert_model.pt')\n",
    "        no_improve = 0\n",
    "        print(f\"Model improved! Saved with F1: {val_f1:.4f}\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Stopping...\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best Val F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ\"):\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        all_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "clf_report = classification_report(all_labels, all_preds, \n",
    "                                 target_names=['Non-Toxic', 'Toxic'],\n",
    "                                 digits=4,\n",
    "                                 output_dict=True)\n",
    "\n",
    "# ================== –í–´–í–û–î –û–¢–ß–ï–¢–ê ==================\n",
    "print(\"=\"*50)\n",
    "print(\"Confusion Matrix Raw Data:\")\n",
    "print(cm)\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(f\"{'':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Non-Toxic':<15} {clf_report['Non-Toxic']['precision']:<10.4f} \"\n",
    "      f\"{clf_report['Non-Toxic']['recall']:<10.4f} \"\n",
    "      f\"{clf_report['Non-Toxic']['f1-score']:<10.4f} \"\n",
    "      f\"{clf_report['Non-Toxic']['support']:<10}\")\n",
    "print(f\"{'Toxic':<15} {clf_report['Toxic']['precision']:<10.4f} \"\n",
    "      f\"{clf_report['Toxic']['recall']:<10.4f} \"\n",
    "      f\"{clf_report['Toxic']['f1-score']:<10.4f} \"\n",
    "      f\"{clf_report['Toxic']['support']:<10}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Accuracy':<15} {'':<30} {clf_report['accuracy']:.4f} {len(all_labels):<10}\")\n",
    "print(f\"{'Macro Avg':<15} {clf_report['macro avg']['precision']:<10.4f} \"\n",
    "      f\"{clf_report['macro avg']['recall']:<10.4f} \"\n",
    "      f\"{clf_report['macro avg']['f1-score']:<10.4f} \"\n",
    "      f\"{clf_report['macro avg']['support']:<10}\")\n",
    "print(f\"{'Weighted Avg':<15} {clf_report['weighted avg']['precision']:<10.4f} \"\n",
    "      f\"{clf_report['weighted avg']['recall']:<10.4f} \"\n",
    "      f\"{clf_report['weighted avg']['f1-score']:<10.4f} \"\n",
    "      f\"{clf_report['weighted avg']['support']:<10}\")\n",
    "\n",
    "# ================== –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø ==================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"F1-Score Analysis:\")\n",
    "print(f\"‚Ä¢ –û–±—â–∏–π F1 (Macro Avg): {clf_report['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"‚Ä¢ –û–±—â–∏–π F1 (Weighted Avg): {clf_report['weighted avg']['f1-score']:.4f}\")\n",
    "print(f\"‚Ä¢ Non-Toxic F1: {clf_report['Non-Toxic']['f1-score']:.4f}\")\n",
    "print(f\"‚Ä¢ Toxic F1: {clf_report['Toxic']['f1-score']:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Toxic', 'Toxic'], \n",
    "            yticklabels=['Non-Toxic', 'Toxic'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏ BERT\n",
    "\n",
    " 1. **–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏**\n",
    "| –ú–µ—Ç—Ä–∏–∫–∞               | –ó–Ω–∞—á–µ–Ω–∏–µ  |\n",
    "|-----------------------|-----------|\n",
    "| **Accuracy**          | 0.9828    |\n",
    "| **Toxic F1**          | 0.9175    |\n",
    "| **Non-Toxic F1**      | 0.9900    |\n",
    "| **False Negatives**   | 189       |\n",
    "| **False Positives**   | 359       |\n",
    "\n",
    "---\n",
    "\n",
    " 2. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏**\n",
    "| –ú–æ–¥–µ–ª—å          | Toxic F1 | FN   | FP   | –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è |\n",
    "|------------------|----------|------|------|----------------|\n",
    "| **LinearSVM**    | 0.7783   | 817  | 562  | 8.4 —Å–µ–∫        |\n",
    "| **LightGBM**     | 0.7645   | -    | -    | 53 —Å–µ–∫         |\n",
    "| **BERT**         | **0.9146** | **189** | **359** | ~8 –º–∏–Ω       |\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ BERT**:\n",
    "- **–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ F1 –¥–ª—è Toxic**: +17.5% –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ LinearSVM.\n",
    "- **–í 4.3 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤** —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (FN ‚Üì —Å 817 –¥–æ 189).\n",
    "- **–ù–∞ 32% –º–µ–Ω—å—à–µ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π** (FP ‚Üì —Å 562 –¥–æ 359).\n",
    "\n",
    "---\n",
    "\n",
    " 3. **–î–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è**\n",
    "| –≠–ø–æ—Ö–∞ | Train Loss | Val F1    | –¢—Ä–µ–Ω–¥               |\n",
    "|-------|------------|-----------|---------------------|\n",
    "| 1     | 0.0943     | 0.8482    | –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è      |\n",
    "| 2     | 0.0842     | **0.9121**| –†–µ–∑–∫–∏–π —Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞|\n",
    "| 3     | 0.0406     | 0.8923    | –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è |\n",
    "| 4     | 0.0209     | **0.9146**| –ù–æ–≤—ã–π –º–∞–∫—Å–∏–º—É–º      |\n",
    "\n",
    "**–ò–Ω—Å–∞–π—Ç—ã**:\n",
    "- –ú–æ–¥–µ–ª—å –±—ã—Å—Ç—Ä–æ —Å—Ö–æ–¥–∏—Ç—Å—è (–ø–∏–∫ F1 –Ω–∞ 2 —ç–ø–æ—Ö–µ).\n",
    "- –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 3 —ç–ø–æ—Ö–µ: Train Loss ‚Üì, Val F1 ‚Üì.\n",
    "- –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞ 4 —ç–ø–æ—Ö–µ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–æ—Å—Ç Train Loss.\n",
    "\n",
    "---\n",
    "\n",
    " 4. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è BERT**\n",
    "1. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è**:\n",
    "   - –í–Ω–µ–¥—Ä–∏—Ç—å **early stopping** –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ Val F1.\n",
    "   - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å **–∑–∞–º–æ—Ä–æ–∑–∫—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤** BERT + –¥–æ–æ–±—É—á–µ–Ω–∏–µ –≤–µ—Ä—Ö–Ω–∏—Ö —Å–ª–æ—ë–≤.\n",
    "2. **–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞**:\n",
    "   - –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ FP/FN.\n",
    "   - –î–æ–±–∞–≤–∏—Ç—å **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞** (–Ω–∞–ø—Ä–∏–º–µ—Ä, –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏).\n",
    "3. **–°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞**:\n",
    "   - –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å **–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏** –∏–ª–∏ **DistilBERT**.\n",
    "   - –£–≤–µ–ª–∏—á–∏—Ç—å –±–∞—Ç—á-—Å–∞–π–∑ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ (—Å–µ–π—á–∞—Å 10.46 –ø—Ä–∏–º–µ—Ä–æ–≤/—Å–µ–∫)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã\n",
    "\n",
    " 1. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π**\n",
    " \n",
    "| –ö—Ä–∏—Ç–µ—Ä–∏–π          | LinearSVM         | LightGBM          | BERT              |\n",
    "|--------------------|-------------------|-------------------|-------------------|\n",
    "| **–°–∫–æ—Ä–æ—Å—Ç—å**       | üü¢ –õ—É—á—à–∞—è (8.4 —Å–µ–∫)| üî¥ –°–∞–º–∞—è –º–µ–¥–ª–µ–Ω–Ω–∞—è | üü° –£–º–µ—Ä–µ–Ω–Ω–∞—è      |\n",
    "| **Toxic F1**       | 0.7783            | 0.7645            | **0.9146**        |\n",
    "| **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å** | üü¢ –í—ã—Å–æ–∫–∞—è     | üü¢ –°—Ä–µ–¥–Ω—è—è        | üî¥ –ù–∏–∑–∫–∞—è         |\n",
    "| **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** | üü¢ –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö | üü° –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ | üî¥ –¢—Ä–µ–±—É–µ—Ç GPU   |\n",
    "\n",
    "---\n",
    "\n",
    " 2. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞**\n",
    "- **–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏**:\n",
    "  - **BERT** ‚Äî –¥–ª—è production, –µ—Å–ª–∏ –∫—Ä–∏—Ç–∏—á–Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –µ—Å—Ç—å GPU-—Ä–µ—Å—É—Ä—Å—ã.\n",
    "  - **LinearSVM** ‚Äî –¥–ª—è MVP –∏–ª–∏ —Å–∏—Å—Ç–µ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –º–æ—â–Ω–æ—Å—Ç—è–º–∏.\n",
    "- **–î–æ—Ä–∞–±–æ—Ç–∫–∏**:\n",
    "  - –î–ª—è BERT: –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ, –±–∞—Ç—á–∏–Ω–≥).\n",
    "  - –î–ª—è LinearSVM: –ø–æ–≤—ã—Å–∏—Ç—å Recall —á–µ—Ä–µ–∑ oversampling –∏–ª–∏ –∞–Ω—Å–∞–º–±–ª–∏.\n",
    "- **–†–∏—Å–∫–∏**:\n",
    "  - **BERT**: –í—ã—Å–æ–∫–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ.\n",
    "  - **LinearSVM**: –†–∏—Å–∫ –ø—Ä–æ–ø—É—Å–∫–∞ —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (FN=817).\n",
    "\n",
    "---\n",
    "\n",
    " 3. **–°—Ç—Ä–∞—Ç–µ–≥–∏—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è**\n",
    "1. **–ü–∏–ª–æ—Ç–Ω–∞—è —Ñ–∞–∑–∞**:\n",
    "   - –ó–∞–ø—É—Å—Ç–∏—Ç—å BERT –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º FP/FN.\n",
    "   - –°—Ä–∞–≤–Ω–∏—Ç—å –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å —Ç–µ–∫—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.\n",
    "2. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**:\n",
    "   - –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.\n",
    "   - –î–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä-–ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –æ—á–µ–≤–∏–¥–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ (–º–∞—Ç—ã, —É–≥—Ä–æ–∑—ã).\n",
    "3. **–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –ø–ª–∞–Ω**:\n",
    "   - –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ **DistilBERT** –∏–ª–∏ **TinyBERT** –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞.\n",
    "   - –í–Ω–µ–¥—Ä–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "---\n",
    "\n",
    "**–§–∏–Ω–∞–ª—å–Ω—ã–π –≤–µ—Ä–¥–∏–∫—Ç**:  \n",
    "BERT ‚Äî –±–µ–∑—É—Å–ª–æ–≤–Ω—ã–π –ª–∏–¥–µ—Ä –ø–æ –∫–∞—á–µ—Å—Ç–≤—É, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.  \n",
    "–î–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –∏–ª–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ LinearSVM –æ—Å—Ç–∞–µ—Ç—Å—è –∂–∏–∑–Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ–π –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–æ–π.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
